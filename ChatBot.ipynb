{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first step is to import the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Pandas- To create/read a dataframe.\n",
    "2.NLTK- To apply natural language operations, in our case for word tokenising.\n",
    "3.Gensim- To create a Word2Vec model. The model will generate feature vectors which act as input to our final LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv file contains sample chat scenarios in question and answer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 7: expected 3 fields, saw 4\\nSkipping line 8: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/rajdesai/Downloads/sample_chat.csv', error_bad_lines=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the questions and answers to lists and combined the list to form our corpus.\n",
    "\n",
    "We have also tokenized the corpus, what we get in return is a list of words which will be fed into Word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=df['Question'].values.tolist()\n",
    "y=df['Answer'].values.tolist()\n",
    "corpus= x+y\n",
    "tok_corp= [nltk.word_tokenize(sent) for sent in corpus]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim’s word2vec expects a sequence of sentences as its input. Each sentence as list of words.\n",
    "\n",
    "Parameters: \n",
    "min_count- it represents the minimum count of words which has to be considered into a model. The default will be 5. We have set it to 1 which means every word will be considered.\n",
    "size- it represents the size of NN layers, which correspond to the “degrees” of freedom the training algorithm has.\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=54, size=32, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would save the model so that it can be initialized at later point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('sample.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatbot Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chat logs available in json format, which we would pre-process to make them suitable as training data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load the previously created word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('sample.bin');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file=open('/Users/rajdesai/Downloads/conversation.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Hi', 'How are you doing?', 'I am doing well.', 'That is good to hear', 'Yes it is.', 'Can I help you with anything?', 'Yes, I have a question.', 'What is your question?', 'Could I borrow a cup of sugar?', \"I'm sorry, but I don't have any.\", 'Thank you anyway', 'No problem']\n"
     ]
    }
   ],
   "source": [
    "print(data[\"conversations\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We create our own corpus of converstions.\n",
    "2. We store the conversations into two different lists.\n",
    "3. Two lists are created because we have to provide the data to the model in sequence.\n",
    "4. Therefore the first list would contain the i,j value and the second list would have i,j+1 values.\n",
    "5. The next task would be to tokenize the sentences.\n",
    "6. Once we have a list of tokens we would input them into our Word2Vec model.\n",
    "7. The output of the model will be a list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cor=data[\"conversations\"];\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "#path2=\"corpus\";\n",
    "\n",
    "for i in range(len(cor)):\n",
    "    for j in range(len(cor[i])):\n",
    "        if j<len(cor[i])-1:\n",
    "            x.append(cor[i][j]);\n",
    "            y.append(cor[i][j+1]);\n",
    "\n",
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(x)):\n",
    "    tok_x.append(nltk.word_tokenize(x[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n",
    "    \n",
    "sentend=np.ones(300, dtype=np.float32) \n",
    "\n",
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    #sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    #sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_y.append(sentvec)           \n",
    "    \n",
    "    \n",
    "for tok_sent in vec_x:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend)    \n",
    "            \n",
    "for tok_sent in vec_y:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of dict object at 0x0000000018DAE288>\n"
     ]
    }
   ],
   "source": [
    "print(data.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12354546,  0.00536549, -0.1516405 ,  0.08004843], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_x[0][0][0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would serialize our data, in this case the two lists of vectors we received as an output from Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would create a LSTM model for training our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is used to build RNN models also it would be running tensorflow in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our list of vectors which were pickle dumped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conversation.pickle','rb') as f:\n",
    "    vec_x,vec_y=pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the vectors into numpy arrays.\n",
    "Though they are slower in operation especially with LSTM, we would use them because of its simplicity to operate and we have limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_x=np.array(vec_x,dtype=np.object)\n",
    "vec_y=np.array(vec_y,dtype=np.object)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to every LSTM layer must be three-dimensional.\n",
    "\n",
    "The three dimensions of this input are:\n",
    "\n",
    "Samples. One sequence is one sample. A batch is comprised of one or more samples.\n",
    "Time Steps. One time step is one point of observation in the sample.\n",
    "Features. One feature is one observation at a time step.\n",
    "\n",
    "This means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 15, 300)\n",
      "(92, 15, 300)\n"
     ]
    }
   ],
   "source": [
    "print(vec_x.shape)\n",
    "print(vec_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have three-dimensional arrays which are ready to be fed into the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters for our model:\n",
    "\n",
    "1.Weights: list of numpy arrays to set as initial weights. The list should have 3 elements, of shapes: [(input_dim, output_dim), (output_dim, output_dim), (output_dim,)].\n",
    "\n",
    "2.return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "\n",
    "3.init: weight initialization function.\n",
    "Glorot normal initializer, also called Xavier normal initializer.\n",
    "It draws samples from a truncated normal distribution centered on 0 with  stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\n",
    "\n",
    "4.activation: activation function, in our case we use the sigmoid function.\n",
    "\n",
    "5.We use cosine_proximity for loss so that the values do not get NaN and become irrelevant to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, input_shape=(15, 300), kernel_initializer=\"glorot_normal\", units=300, recurrent_initializer=\"glorot_normal\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, input_shape=(15, 300), kernel_initializer=\"glorot_normal\", units=300, recurrent_initializer=\"glorot_normal\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, input_shape=(15, 300), kernel_initializer=\"glorot_normal\", units=300, recurrent_initializer=\"glorot_normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, input_shape=(15, 300), kernel_initializer=\"glorot_normal\", units=300, recurrent_initializer=\"glorot_normal\")`\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 15, 300)\n",
      "(73, 15, 300)\n",
      "(19, 15, 300)\n",
      "(19, 15, 300)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x00000000165E6F60>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 4s - loss: -0.0019 - acc: 9.1324e-04 - val_loss: -0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 0s - loss: -0.0020 - acc: 9.1324e-04 - val_loss: -0.0020 - val_acc: 0.0281\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0192 - val_loss: -0.0021 - val_acc: 0.0456\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0338 - val_loss: -0.0021 - val_acc: 0.0491\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0384 - val_loss: -0.0021 - val_acc: 0.0632\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0521 - val_loss: -0.0021 - val_acc: 0.0667\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0612 - val_loss: -0.0021 - val_acc: 0.0526\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 0s - loss: -0.0021 - acc: 0.0621 - val_loss: -0.0021 - val_acc: 0.0456\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0584 - val_loss: -0.0022 - val_acc: 0.0456\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0575 - val_loss: -0.0022 - val_acc: 0.0386\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0475 - val_loss: -0.0022 - val_acc: 0.0386\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0475 - val_loss: -0.0022 - val_acc: 0.0281\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0402 - val_loss: -0.0022 - val_acc: 0.0281\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0429 - val_loss: -0.0022 - val_acc: 0.0281\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0420 - val_loss: -0.0022 - val_acc: 0.0491\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0347 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0338 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0338 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0347 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0356 - val_loss: -0.0022 - val_acc: 0.0246\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 0s - loss: -0.0022 - acc: 0.0420 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0511 - val_loss: -0.0022 - val_acc: 0.0421\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0511 - val_loss: -0.0023 - val_acc: 0.0421\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0511 - val_loss: -0.0023 - val_acc: 0.0421\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0575 - val_loss: -0.0023 - val_acc: 0.0526\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0630 - val_loss: -0.0023 - val_acc: 0.0456\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0584 - val_loss: -0.0023 - val_acc: 0.0456\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0584 - val_loss: -0.0023 - val_acc: 0.0491\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0575 - val_loss: -0.0023 - val_acc: 0.0596\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0539 - val_loss: -0.0023 - val_acc: 0.0561\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0539 - val_loss: -0.0023 - val_acc: 0.0596\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 0s - loss: -0.0023 - acc: 0.0548 - val_loss: -0.0024 - val_acc: 0.0561\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0557 - val_loss: -0.0024 - val_acc: 0.0491\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0603 - val_loss: -0.0024 - val_acc: 0.0456\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0575 - val_loss: -0.0024 - val_acc: 0.0456\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0612 - val_loss: -0.0024 - val_acc: 0.0526\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0594 - val_loss: -0.0024 - val_acc: 0.0491\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0521 - val_loss: -0.0024 - val_acc: 0.0456\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0557 - val_loss: -0.0024 - val_acc: 0.0491\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0548 - val_loss: -0.0024 - val_acc: 0.0491\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0557 - val_loss: -0.0024 - val_acc: 0.0526\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0539 - val_loss: -0.0024 - val_acc: 0.0526\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0612 - val_loss: -0.0023 - val_acc: 0.0421\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0557 - val_loss: -0.0024 - val_acc: 0.0561\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0584 - val_loss: -0.0024 - val_acc: 0.0526\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0603 - val_loss: -0.0024 - val_acc: 0.0421\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0584 - val_loss: -0.0024 - val_acc: 0.0386\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0575 - val_loss: -0.0024 - val_acc: 0.0561\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0621 - val_loss: -0.0024 - val_acc: 0.0561\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 0s - loss: -0.0024 - acc: 0.0612 - val_loss: -0.0024 - val_acc: 0.0386\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=50,validation_data=(x_test, y_test))\n",
    "model.save('LSTM5000.h5');          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(x_test) \n",
    "mod = gensim.models.Word2Vec.load('sample.bin'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to take text as input and try to generate an appropriate reply with the help of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=load_model('LSTM5000.h5')\n",
    "mod = gensim.models.Word2Vec.load('/Users/rajdesai/Downloads/doc2vec.bin');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query:h\n",
      "so they they but but have have apart duluth duluth duluth duluth duluth duluth duluth\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    x=input(\"Enter your query:\");\n",
    "    if x == \"1\":\n",
    "        print(\"Thanks for chatting\")\n",
    "        break\n",
    "    else:\n",
    "        sentend=np.ones((300,),dtype=np.float32) \n",
    "\n",
    "        sent=nltk.word_tokenize(x.lower())\n",
    "        sentvec = [mod[w] for w in sent if w in mod.wv.vocab]\n",
    "\n",
    "        sentvec[14:]=[]\n",
    "        sentvec.append(sentend)\n",
    "        if len(sentvec)<15:\n",
    "            for i in range(15-len(sentvec)):\n",
    "                sentvec.append(sentend) \n",
    "        sentvec=np.array([sentvec])\n",
    "    \n",
    "        predictions = model.predict(sentvec)\n",
    "        outputlist=[mod.most_similar([predictions[0][i]])[0][0] for i in range(15)]\n",
    "        output=' '.join(outputlist)\n",
    "        print(output)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
